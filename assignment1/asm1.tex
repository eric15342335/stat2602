\documentclass{article}
\usepackage{amssymb, amsthm, enumitem, multicol}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in,top=0.6in,bottom=0.6in]{geometry}
\usepackage[fleqn]{amsmath}

% \usepackage{minted}
% Add "-shell-escape", to vscode:
% Open settings, search for "latex-workshop.latex.tools", click "Edit in settings.json", add "-shell-escape" to the pdflatex command.

% Independent symbol
\newcommand{\independent}{\perp\!\!\!\perp} 

\title{STAT2602 Assignment 1}
\author{Cheng Ho Ming, Eric (3036216734) [Section 1A, 2024]}

\begin{document}
\maketitle

\begin{enumerate}
% Question 1
\item
    \begin{enumerate}[label=(\roman*)]
    \item $\begin{aligned}[t]
    \text{The cumulative density function of } X \text{ is:} \\
    F(x) &= Pr(X \leq x) = \sum_{i=1}^{x} f(i) \\
    &= \sum_{i=1}^{x} 2*(\frac{1}{3})^x \\
    &= 2*(\frac{1}{3})^1 + 2*(\frac{1}{3})^2 + ... + 2*(\frac{1}{3})^x \\
    &= 2*(\frac{\frac{1}{3}*(1-(\frac{1}{3})^x)}{1-\frac{1}{3}}) \\
    &= \frac{2*\frac{1}{3}}{\frac{2}{3}}*(1-(\frac{1}{3})^x) \\
    &= 1-\frac{1}{3^x} \text{ for } x \text{ = 1,2,3,}\dots \\
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{The moment generating function (m.g.f.) of } X \text{ is:} \\
    M_X(t) &= E(e^{tX}) = \sum_{x=1}^{\infty} e^{tx}f(x) \\
    &= \sum_{x=1}^{\infty} e^{tx}*2*(\frac{1}{3})^x \\
    &= 2*\sum_{x=1}^{\infty} (\frac{e^t}{3})^x \text{ for } e^t < 3 \\
    &= 2*\frac{\frac{e^t}{3}}{1-\frac{e^t}{3}} \text{ for } t < \ln 3 \text{ (Geometric series)}\\
    \therefore M_X(t) &= \frac{2e^t}{3-e^t} \text{ for } t < \ln 3
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{Using the moment generating function, } \\
    E(X) &= M_X'(0) \\
    &= \frac{d}{dt}(\frac{2e^t}{3-e^t})|_{t=0} \\
    &= \frac{(3-e^t)(2e^t)-(2e^t)(-e^t)}{(3-e^t)^2}|_{t=0} \\
    &= \frac{(3-1)(2)-(2)(-1)}{(3-1)^2} \\
    &= \frac{6}{4} \\
    &= \frac{3}{2} \\
    Var(X) &= M_X''(0) - (M_X'(0))^2 \\
    &= \frac{d^2}{dt^2}(\frac{2e^t}{3-e^t})|_{t=0} - (\frac{3}{2})^2 \\
    &= \frac{d}{dt}(\frac{(3-e^t)(2e^t)-(2e^t)(-e^t)}{(3-e^t)^2})|_{t=0} - \frac{9}{4} \\
    &= \frac{d}{dt}(\frac{6e^t-2e^{2t}+2e^{2t}}{(3-e^t)^2})|_{t=0} - \frac{9}{4} \\
    &= \frac{d}{dt}(\frac{6e^t}{(3-e^t)^2})|_{t=0} - \frac{9}{4} \\
    &= (\frac{(3-e^t)^2*6e^t-6e^t*2(3-e^t)(-e^t)}{(3-e^t)^4})|_{t=0} - \frac{9}{4} \\
    &= \frac{(3-1)^2*6-6*2(3-1)(-1)}{(3-1)^4} - \frac{9}{4} \\
    &= \frac{4*6+6*2*2}{(3-1)^4} - \frac{9}{4} \\
    &= \frac{24+24}{16} - \frac{9}{4} \\
    &= 3 - \frac{9}{4} \\
    &= \frac{3}{4}
    \end{aligned}$
    \end{enumerate}

% Question 2
\item
    \begin{enumerate}[label=(\roman*)]
    \item 
    $\text{Since } X_1, X_2, ..., X_n \text{ are } \sim_{\text{i.i.d.}}Gamma(3,\theta) \text{, the moment generating function (m.g.f.) of } Y \text{ is:} \\$
    $\begin{aligned}[t]
    M_Y(t) &= M_{X_1}(t) * M_{X_2}(t) * ... * M_{X_n}(t) \\
    &= (\frac{\theta}{\theta-t})^3 * (\frac{\theta}{\theta-t})^3 * ... * (\frac{\theta}{\theta-t})^3 \\
    &= (\frac{\theta}{\theta-t})^{3n} \\
    &= \text{m.g.f. of } Gamma(3n, \theta) \\
    \therefore Y &\sim Gamma(3n, \theta) \text{ where } \theta > 0.
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{Using the moment generating function, } \\
    E(cY) &= cE(Y) = cM_Y'(0) \\
    \theta^{-1} &= c(\frac{d}{dt}(\frac{\theta}{\theta-t})^{3n})|_{t=0} \\
    \theta^{-1} &= c(3n(\frac{\theta}{\theta-t})^{3n-1}\frac{\theta}{(\theta-t)^2})|_{t=0} \\
    \theta^{-1} &= c*3n*1*\frac{1}{\theta} \\
    1 &= 3nc \\
    c &= \frac{1}{3n}
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{The moment generating function (m.g.f.) of } 3\theta Y+1 \text{ is:} \\
    % Property 2.1(3) M_{aX+b}(t) = e^{bt}M_X(at)
    M_{3\theta Y+1}(t) &= E(e^{t(3\theta Y+1)}) \\
    &= e^t E(e^{3\theta Yt}) \\
    &= e^t M_{Y}(3\theta t) \\
    &= e^t (\frac{\theta}{\theta-3\theta t})^{3n} \\
    &= e^t (\frac{1}{1-3t})^{3n} \text{ for } t < \frac{1}{3} \\
    \end{aligned}$
    \end{enumerate}

\item
    \begin{enumerate}[label=(\roman*)]
    \item $\begin{aligned}[t]
    \text{The mean of } X \text{ is:} \\
    E(X) &= M_X'(0) \\
    &= (-\frac{3}{4}e^{-3t}+\frac{e^t}{4})|_{t=0} \\
    &= -\frac{3}{4} + \frac{1}{4} \\
    &= -\frac{1}{2} \\
    \text{The variance of } X \text{ is:} \\
    Var(X) &= M_X''(0) - (M_X'(0))^2 \\
    &= (\frac{9}{4}e^{-3t}+\frac{e^t}{4})|_{t=0} - (-\frac{1}{2})^2 \\
    &= \frac{9}{4} + \frac{1}{4} - \frac{1}{4} \\
    &= \frac{9}{4}
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{By the moment generating function, } \\
    E(e^{tX}) &= M_X(t) \\
    \sum_{x \in X(\Omega)} e^{tx} f(x) &= \frac{1}{4}e^{-3t} + \frac{1}{2} + \frac{1}{4}e^t \\
    &= \frac{1}{4}e^{-3t} + \frac{1}{2}e^{0t} + \frac{1}{4}e^{1t} \\
    \end{aligned}$

    $\begin{aligned}[t]
    \text{Comparing the coefficients of } e^{-3t}, e^{0t}, e^{1t} & \text{ on both sides, we have:} \\
    \therefore f(x) = \begin{cases}
        \frac{1}{4} & \text{if } x = -3 \\
        \frac{1}{2} & \text{if } x = 0 \\
        \frac{1}{4} & \text{if } x = 1 \\
        0 & \text{otherwise}
    \end{cases}
    \text{, which is the pmf of } X. \\
    \text{Checking the expression of the pmf by (i), we have:} \\
    E(X) &= -3*\frac{1}{4} + 0*\frac{1}{2} + 1*\frac{1}{4} \\
    &= -\frac{1}{2} \\
    Var(X) &= E(X^2) - (E(X))^2 \\
    &= ((-3)^2*\frac{1}{4} + 0^2*\frac{1}{2} + 1^2*\frac{1}{4}) - (-\frac{1}{2})^2 \\
    &= (\frac{9}{4} + \frac{1}{4}) - \frac{1}{4} \\
    &= \frac{9}{4} \\
    \text{Which matches the results in (i).} \\
    \end{aligned}$
    \end{enumerate}

    
\item
    \begin{enumerate}[label=(\roman*)]
    \item $\begin{aligned}[t]
    \text{The empirical distribution function is:} \\
    F_{10}(x) = \begin{cases}
        0 & \text{for } x < 0 \\
        0.1 & \text{for } 0 \leq x < 1 \\
        0.2 & \text{for } 1 \leq x < 2 \\
        0.4 & \text{for } 2 \leq x < 3 \\
        0.6 & \text{for } 3 \leq x < 4 \\
        0.7 & \text{for } 4 \leq x < 6 \\
        0.9 & \text{for } 6 \leq x < 7 \\
        1 & \text{for } x >= 7 \\
    \end{cases}
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{Using the empirical } & \text{distribution, we have:} \\
    Pr(X<=4) &= F(4) \approx F_{10}(4) = 0.7 \\
    Pr(4<X<7) &= Pr(4<X<=6) \\
    & \approx F_{10}(6) - F_{10}(4) \\
    &= 0.9 - 0.7 \\
    &= 0.2
    \end{aligned}$
    \end{enumerate}

\item
    \begin{enumerate}[label=(\roman*)]
    \item $\begin{aligned}[t]
    \because \xi_1 \text{ and } \xi_2 \text{ are independent,} \\
    \therefore \text{the moment generating function of } X \text{ is:} \\
    M_X(t) &= M_{\xi_1}(t) * M_{\xi_2}(t) \\
    &= \exp(\theta t + \frac{1}{2} t^2) * \exp(\lambda \theta t + \frac{1}{2}\lambda^2 t^2) \\
    &= \exp(\theta t + \frac{1}{2} t^2 + \lambda \theta t + \frac{1}{2}\lambda^2 t^2) \\
    &= \exp((\theta + \lambda \theta) t + \frac{1}{2} (1 + \lambda^2) t^2) \text{ for } t \in \mathbb{R} \\
    \end{aligned}$
    
    \item $\begin{aligned}[t]
    M_X(t) &= \exp((\theta + \lambda \theta) t + \frac{1}{2} (1 + \lambda^2) t^2) \\
    M_X'(t) &= (\theta + \lambda \theta + (1 + \lambda^2) t) M_X(t) \\
    M_X''(t) &= (\theta + \lambda \theta + (1 + \lambda^2) t)^2 M_X(t) + M_X(t) * (1 + \lambda^2) \\
    M_X'''(t) &= (\theta + \lambda \theta + (1 + \lambda^2) t)^3 * M_X(t) + M_X(t) * 2(\theta + \lambda \theta + (1 + \lambda^2) t) * (1 + \lambda^2) \\
    & \ \ \ \ + (1 + \lambda^2) (\theta + \lambda \theta + (1 + \lambda^2) t) M_X(t) \\
    \therefore E(X^3) &= M_X'''(0) \\
    &= (\theta + \lambda \theta)^3 + 2*(\theta + \lambda \theta)(1 + \lambda^2) + (1 + \lambda^2) (\theta + \lambda \theta) \\
    &= \theta^3 (1 + \lambda)^3 + 2 * \theta (1 + \lambda) (1 + \lambda^2) + \theta (1 + \lambda^2) (1 + \lambda) \\
    &= \theta (1 + \lambda) (\theta^2 (1 + \lambda)^2 + 2(1 + \lambda^2) + (1 + \lambda^2)) \\
    &= \theta (1 + \lambda) (\theta^2 (1 + \lambda)^2 + 3(1 + \lambda^2))
    \end{aligned}$

    \item $
    \text{Given the moment generating function of X is:} \\
    M_X(t) = \exp((\theta + \lambda \theta) t + \frac{1}{2} (1 + \lambda^2) t^2) \\
    \therefore X \sim N(\theta + \lambda \theta, 1 + \lambda^2)
    $
    \end{enumerate}

\item 
    $\begin{aligned}[t]
    \text{Since } X \text{ is a continuous random variable,} \\
    \text{Mean of } X = E(X) = \mu &= \int_{0}^2 x \frac{x^3}{4} dx \\
    &= \frac{1}{4} \int_{0}^2 x^4 dx \\
    &= \frac{1}{4} [\frac{1}{5} x^5]_{0}^2 \\
    &= \frac{1}{4} * \frac{1}{5} * 2^5 \\
    &= \frac{32}{20} \\
    &= \frac{8}{5} \\
    \text{Variance of } X = Var(X) = \sigma^2 &= E(X^2) - (E(X))^2 \\
    &= \int_{0}^2 x^2 \frac{x^3}{4} dx - (\frac{8}{5})^2 \\
    &= \frac{1}{4} \int_{0}^2 x^5 dx - \frac{64}{25} \\
    &= \frac{1}{4} [\frac{1}{6} x^6]_{0}^2 - \frac{64}{25} \\
    &= \frac{1}{4} * \frac{1}{6} * 2^6 - \frac{64}{25} \\
    &= \frac{64}{24} - \frac{64}{25} \\
    &= \frac{8}{75} \\
    \text{Using central limit theorem, we have:} \\
    Pr(1.2 \leq \bar{X} \leq 1.6) &= Pr(\frac{\sqrt{n}(1.2 - \mu)}{\sigma} \leq \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \leq \frac{\sqrt{n}(1.6 - \mu)}{\sigma}) \\
    &= Pr(\frac{\sqrt{25}(1.2 - \frac{8}{5})}{\sqrt{8/75}} \leq Z \leq \frac{\sqrt{25}(1.6 - \frac{8}{5})}{\sqrt{8/75}}) \\
    \because & \ Z \sim N(0, 1), \\
    \therefore Pr(1.2 \leq \bar{X} \leq 1.6) &= Pr(-6.123724 \leq Z \leq 0) \\
    & \approx 0.5
    \end{aligned}$

\item 
    \begin{enumerate}[label=(\roman*)]
    \item $\begin{aligned}[t]
    \text{Since } & \text{X is a continuous random variable, the cumulative distribution function of Y is:} \\
    F_Y(y) &= Pr(Y \leq y) \\
    &= Pr(X_1 <= y , X_2 <= y , X_3 <= y , \dots , X_{12} <= y) \\
    & \text{ (as } Y=X_{(12)} \text{which is the 12th smallest value in the sample)} \\
    &= [Pr(X_1 <= y)]^{12} \text{ (as } X_1, X_2, \dots , X_{12} \text{ are independent)} \\
    &= [F_X(y)]^{12} \\
    \ \ & \text{as } X \sim U(0, 1) \\
    &= y^{12} \text{ for } y \in (0, 1) \\
    & \text{Therefore, the probability density function of Y is:} \\
    f_y(y) &= \frac{d}{dy} F_Y(y) \\
    &= \frac{d}{dy} y^{12} \\
    &= 12y^{11} \text{ for } y \in (0, 1) \\
    \end{aligned}$

    \item $\begin{aligned}[t]
    \text{Given } Z &= ( \sum_{i=1}^{12} X_i ) - 6, \text{ the moment generating function of Z is:} \\
    M_Z(t) &= E(e^{tZ}) \\
    &= E(e^{t( \sum_{i=1}^{12} X_i ) - 6t}) \\
    &= e^{-6t} * E(e^{t \sum_{i=1}^{12} X_i} ) \\
    &= e^{-6t} * E(e^{tX_1} * e^{tX_2} * \dots * e^{tX_{12}}) \\
    &= e^{-6t} * [E(e^{tX_1})]^{12} \text{ (as } X_1, X_2, \dots , X_{12} \text{ are independent)} \\
    &= e^{-6t} * [M_X(t)]^{12} \\
    \because M_X(t) &= E(e^{tX}) \\
    &= \int_{0}^1 e^{tx} dx \\
    &= \begin{cases}
        \frac{e^t - 1}{t} & \text{if } t \neq 0 \\
        1 & \text{if } t = 0 \\
    \end{cases} \\
    \therefore M_Z(t) &= \begin{cases}
        e^{-6t} \frac{(e^{t} - 1)^{12}}{t^{12}} & \text{if } t \neq 0 \\
        e^{-6t} & \text{if } t = 0 \\
    \end{cases} \\
    &= \begin{cases}
        e^{-6t} \frac{(e^{t} - 1)^{12}}{t^{12}} & \text{if } t \neq 0 \\
        1 & \text{if } t = 0 \\
    \end{cases} \\
    \end{aligned}$

    \item
    Given $X_1, X_2, \dots, X_{12} \overset{i.i.d.}{\sim} U(0, 1)$, \\
    Let $\bar{X} = (\sum_{i=1}^{12} X_i) / 12$, then:
    \begin{align*}
        \mu = E(X_1) = 0.5 \\
        \sigma = \sqrt{Var(X_1)} = \sqrt{\frac{1}{12}}
    \end{align*}

    We have:
    \begin{align*}
        \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} &= \frac{\sqrt{12}(\bar{X} - 0.5)}{\sqrt{\frac{1}{12}}} \\
        &= 12 (\bar{X} - 0.5) \\
        &= 12 \bar{X} - 6 \\
        &= Z
    \end{align*}

    By central limit theorem,
    $\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \to_d N(0, 1) \text{ as } n \to \infty.$ \\
    Since $n = 12$ is  large enough, we can say that $Z$ is approximately distributed as standard normal $N(0, 1)$.
    \end{enumerate}

\item 
    \begin{enumerate}[label=(\roman*)]
    \item 
    Let $P(n)$ be the predicate ``$\bar{X}_n = \frac{1}{n} (\sum_{i=1}^{n} X_i) \overset{d}{=} X_1 $'' for $n \in \mathbb{Z}^+$, where $X \overset{d}{=} Y$ means X and Y are in the same distribution. \\
    $\begin{aligned}[t]
    \text{For } n = 1, \text{ L.H.S. } &=  \bar{X}_1 = X_1 \\
    \text{, R.H.S. } &= X_1 \\
    \because \text{L.H.S. } & \overset{d}{=} \text{ R.H.S. }, \\
    \therefore P(1) \text{ is} & \text{ true.}
    \end{aligned}$

    Assume P(k) is true for some $k \in \mathbb{Z}^+$, i.e. ``$\bar{X}_k = \frac{1}{k} (\sum_{i=1}^{k} X_i) \overset{d}{=} X_1 $'', \\
    $\begin{aligned}[t]
    \text{For } n = k + 1, \\
    \text{L.H.S. } &= \bar{X}_{k+1} = \frac{1}{k+1} (\sum_{i=1}^{k+1} X_i) \\
    &= \frac{1}{k+1} (X_{k+1} + \sum_{i=1}^{k} X_i) \\
    &= \frac{1}{k+1} (X_{k+1} + k \bar{X}_k) \\
    &= \frac{1}{k+1} X_{k+1} + \frac{k}{k+1} \bar{X}_k \\
    & \text{Let } p = \frac{1}{k+1} \text{ and } 1-p = 1 - \frac{1}{k+1} = \frac{k}{k+1}, \\
    & \text{given that } T = pU+(1-p)V \text{ is also distributed as Cauchy,} \\
    &= p X_{k+1} + (1-p) \bar{X}_k \\
    & \overset{d}{=} p X_{k+1} + (1-p) X_1  \text{    (by induction hypothesis)} \\
    & \overset{d}{=} p X_{1} + (1-p) X_1 \text{ (given that } X_1 \overset{d}{=} X_{k+1}) \\
    & \overset{d}{=} p X_{1} - p X_{1} + X_1 \\
    & \overset{d}{=} X_1 \\
    \end{aligned}$

    $\because P(k+1)$ is true if $P(k)$ is true, \\
    $\therefore $ By the principle of mathematical induction, $P(n)$ is true for all $n \in \mathbb{Z}^+$. \\
    Therefore, $\bar{X}_n = \bar{X} = \frac{1}{n} (\sum_{i=1}^{n} X_i)$ has the same distribution as $X_1$, which is Cauchy.

    $\begin{aligned}[t]
    E(X_1) &= \int_{-\infty}^{\infty} \frac{x}{\pi(1+x^2)} dx \\
    &= \frac{1}{\pi} \int_{-\infty}^{\infty} \frac{x}{1+x^2} dx \\
    &= \frac{1}{\pi} (\lim_{y \to \infty} \int_{-y}^{0} \frac{x}{1+x^2} dx + \lim_{y \to \infty} \int_{0}^{y} \frac{x}{1+x^2} dx) \\
    % from graph
    &= -\infty + \infty \\
    \end{aligned}$
    
    $\because$ the integral is undefined, \\
    $\therefore E(X_1)$ does not exist. \\
    Therefore, $\lim_{n \to \infty} \bar{X}_n$ does not exist, so $\lim_{n \to \infty} Pr(|\bar{X}_n - X| \geq \epsilon)$ does not exist for any $\epsilon > 0$.

    \item
    For (weak) law of large numbers, it requires the sequence of independent and identically distributed random variables to have *FINITE* mean. \\
    Since the Cauchy distribution does not have a finite mean, the (weak) law of large numbers does not apply to the Cauchy distribution.
    \end{enumerate}
\end{enumerate}

\end{document}
